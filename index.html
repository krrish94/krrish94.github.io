<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Krishna Murthy Jatavallabhula</title> <meta name="author" content="Krishna Murthy Jatavallabhula"/> <meta name="description" content="Krishna Murthy - Research scientist, Meta "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://krrish94.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Krishna</span> Murthy Jatavallabhula </h1> <p class="desc">World Models for Robots</p> </header> <article> <div class="container"> <div class="row"> <div class="col-sm-3"> <img src="assets/img/prof_pic.png" width="100%" style="border-radius:15px" alt="A photograph of Krishna"> <p align="center"> <b><a href="mailto:kmj@jhu.edu">Email</a></b> | <b><a href="https://scholar.google.co.uk/citations?user=kcr8134AAAAJ" target="_blank" rel="noopener noreferrer">Google Scholar</a></b> <br> <b><a href="assets/pdf/CV.pdf">CV</a></b> | <b><a href="assets/pdf/thesis_krishna.pdf">PhD Thesis</a></b> <br> <b><a href="https://github.com/krrish94" target="_blank" rel="noopener noreferrer">GitHub</a></b> | <b><a href="https://www.linkedin.com/in/krrish94/" target="_blank" rel="noopener noreferrer">LinkedIn </a></b> </p> </div> <div class="col-sm"> <div class="clearfix"> <p>I am an AI research scientist at <a href="https://ai.meta.com/research/" target="_blank" rel="noopener noreferrer">Meta</a> and an incoming assistant professor with the <a href="https://www.cs.jhu.edu/" target="_blank" rel="noopener noreferrer">CS department</a> at <a href="https://www.jhu.edu/" target="_blank" rel="noopener noreferrer">Johns Hopkins University</a>.</p> <p>I strive to build full-stack robotic systems that perceive, reason, and act with human-level efficiency, ultimately surpassing them. My work lies at the <em>perception-action interface</em>, tackling both how robots should represent the world around them, and how they use it for action. This draws inspiration from several adjacent fields, including computer vision, machine learning, and cognitive science.</p> <p>Previously, I was a postdoc at <a href="https://www.csail.mit.edu/" target="_blank" rel="noopener noreferrer">MIT CSAIL</a> with <a href="https://web.mit.edu/torralba/www/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a> and <a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="noopener noreferrer">Josh Tenenbaum</a>. I completed my PhD at <a href="https://www.umontreal.ca/" target="_blank" rel="noopener noreferrer">Universite de Montreal</a> and <a href="https://mila.quebec/en/" target="_blank" rel="noopener noreferrer">Mila</a>, advised by <a href="http://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a>. My work has been recognized with PhD fellowship awards from NVIDIA and Google, and a best-paper award from IEEE RAL.</p> </div> </div> </div> </div> <div class="news"> <h2>News</h2> <div class="table-responsive" style="max-height: 20vh"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 13, 2025</th> <td> Long overdue webpage update, featuring work through mid-2025. </td> </tr> <tr> <th scope="row">Jun 13, 2025</th> <td> By Fall <strong>2026</strong>, I will join the CS department at Johns Hopkins University as an assistant professor. </td> </tr> <tr> <th scope="row">Sep 16, 2024</th> <td> I joined FAIR, Meta as an AI research scientist. </td> </tr> <tr> <th scope="row">Sep 14, 2024</th> <td> I will serve as an area chair for CVPR 2025. </td> </tr> <tr> <th scope="row">Sep 13, 2024</th> <td> I completed an eventful 2.5-year postdoc stint at MIT CSAIL. </td> </tr> <tr> <th scope="row">Jun 1, 2024</th> <td> Serving as OpenReview chair for <a href="https://corl.org/" target="_blank" rel="noopener noreferrer">CoRL 2024</a> </td> </tr> <tr> <th scope="row">Mar 1, 2024</th> <td> Speaking at the <a href="https://robotics.umd.edu/futureleaders" target="_blank" rel="noopener noreferrer">UMD/Microsoft future leaders in robotics and AI</a> seminar series </td> </tr> <tr> <th scope="row">Jan 29, 2024</th> <td> 6 papers accepted to ICRA 2024. </td> </tr> <tr> <th scope="row">Jan 23, 2024</th> <td> Serving as associate editor for IROS and RA-L. </td> </tr> <tr> <th scope="row">Sep 28, 2023</th> <td> Another webpage update, featuring new work, including <a href="https://concept-graphs.github.io" target="_blank" rel="noopener noreferrer">ConceptGraphs</a>. </td> </tr> <tr> <th scope="row">Feb 12, 2023</th> <td> Long overdue webpage update, including the featured <a href="https://concept-fusion.github.io/" target="_blank" rel="noopener noreferrer">Conceptfusion</a> work. </td> </tr> <tr> <th scope="row">Mar 15, 2022</th> <td> I moved to MIT to start my potsdoc with <a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="noopener noreferrer">Josh Tenenbaum</a> and <a href="https://web.mit.edu/torralba/www/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a>. </td> </tr> <tr> <th scope="row">Mar 11, 2022</th> <td> Got my PhD with grade: exceptional! </td> </tr> <tr> <th scope="row">Feb 2, 2022</th> <td> Serving as associate editor for IROS 2022 </td> </tr> <tr> <th scope="row">Sep 15, 2021</th> <td> Organizing workshops <a href="http://montrealrobotics.ca/diff3d/" target="_blank" rel="noopener noreferrer">Diff3D</a> ICCV 2021, and the <a href="https://physical-reasoning.github.io/" target="_blank" rel="noopener noreferrer">PRIBR</a> at Neurips 2021. </td> </tr> <tr> <th scope="row">Sep 5, 2021</th> <td> Teaching the <a href="http://www.cim.mcgill.ca/~derek/ecse446.html" target="_blank" rel="noopener noreferrer">realistic/advanced image synthesis</a> class at McGill university (Fall 2021). </td> </tr> <tr> <th scope="row">Jan 22, 2021</th> <td> Awarded a <a href="https://research.google/outreach/phd-fellowship/" target="_blank" rel="noopener noreferrer">Google PhD fellowship</a> (declined) </td> </tr> <tr> <th scope="row">Dec 21, 2020</th> <td> Organizing the <a href="https://rethinkingmlpapers.github.io" target="_blank" rel="noopener noreferrer">rethinking ML papers</a> workshop at <a href="https://iclr.cc" target="_blank" rel="noopener noreferrer">ICLR 2021</a> </td> </tr> <tr> <th scope="row">Dec 1, 2020</th> <td> Honored to have received an <a href="https://blogs.nvidia.com/blog/2020/12/04/graduate-fellowships-gpu-computing-research/" target="_blank" rel="noopener noreferrer">NVIDIA graduate fellowship</a> for 2021-22 </td> </tr> <tr> <th scope="row">Nov 10, 2020</th> <td> gradSLAM is available as an open-source PyTorch framework <a href="https://github.com/gradslam/gradslam" target="_blank" rel="noopener noreferrer">here</a> </td> </tr> <tr> <th scope="row">Sep 3, 2020</th> <td> Organizing the <a href="https://montrealrobotics.ca/robotlearningseries" target="_blank" rel="noopener noreferrer">robot learning seminar series</a> at Mila </td> </tr> <tr> <th scope="row">Sep 1, 2020</th> <td> Organizing the <a href="https://montrealrobotics.ca/diffcvgp/" target="_blank" rel="noopener noreferrer">differentiable vision, graphics, physics</a> workshop at Neurips 2020 </td> </tr> <tr> <th scope="row">Jul 6, 2020</th> <td> Selected to the <a href="https://sites.google.com/view/rsspioneers2020/home" target="_blank" rel="noopener noreferrer">RSS pioneers cohort for 2020</a> </td> </tr> <tr> <th scope="row">Jun 5, 2020</th> <td> Our paper, <a href="https://ieeexplore.ieee.org/document/8936918" target="_blank" rel="noopener noreferrer">MapLite</a>, named <a href="https://www.ieee-ras.org/publications/ra-l/ra-l-paper-awards" target="_blank" rel="noopener noreferrer">best paper, IEEE RAL 2019</a>. </td> </tr> <tr> <th scope="row">Feb 12, 2020</th> <td> Our paper on fully differentiable dense SLAM will be (virtually) presented at <a href="https://www.icra2020.org/" target="_blank" rel="noopener noreferrer">ICRA 2020</a> </td> </tr> <tr> <th scope="row">Nov 14, 2019</th> <td> Released NVIDIA <a href="https://github.com/NVIDIAGameWorks/kaolin/" target="_blank" rel="noopener noreferrer">Kaolin</a>: a 3D deep learning library </td> </tr> </table> </div> </div> <div class="publications"> <h2>Featured publications</h2> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/conceptgraphs.gif" alt="A gif depicting the 3D mapping process implemented as part of ConceptGraphs."></div> <div class="col-sm"></div> <div id="conceptgraphs" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</p></div> <div class="author"> <a href="https://georgegu1997.github.io/" target="_blank" rel="noopener noreferrer">Qiao Gu</a>*, <a href="https://www.alihkw.com/" target="_blank" rel="noopener noreferrer">Alihusein Kuwajerwala</a>*, <a href="https://sachamorin.github.io/" target="_blank" rel="noopener noreferrer">Sacha Morin</a>*,  <em>Krishna Murthy Jatavallabhula</em>*, <a href="https://bipashasen.github.io/" target="_blank" rel="noopener noreferrer">Bipasha Sen</a>, <a href="https://skymanaditya1.github.io/" target="_blank" rel="noopener noreferrer">Aditya Agarwal</a>, <a href="https://www.jhuapl.edu/work/our-organization/research-and-exploratory-development/red-staff-directory/corban-rivera" target="_blank" rel="noopener noreferrer">Corban Rivera</a>, <a href="https://scholar.google.com/citations?user=92bmh84AAAAJ" target="_blank" rel="noopener noreferrer">William Paul</a>, <a href="https://mila.quebec/en/person/kirsty-ellis/" target="_blank" rel="noopener noreferrer">Kirsty Ellis</a>, <a href="https://engineering.jhu.edu/faculty/rama-chellappa/" target="_blank" rel="noopener noreferrer">Rama Chellappa</a>, <a href="https://people.csail.mit.edu/ganchuang/" target="_blank" rel="noopener noreferrer">Chuang Gan</a>, <a href="https://celsodemelo.net/" target="_blank" rel="noopener noreferrer">Celso Miguel de Melo</a>, <a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="noopener noreferrer">Joshua B. Tenenbaum</a>, <a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a>, <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a>, and <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a> </div> <div class="periodical"> <em>ICRA</em> 2024 </div> <div class="links"> <a href="https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://concept-graphs.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/concept-graphs/concept-graphs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/mRhNkQwRYnc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts.</p> </div> <div class="contrib hidden"> <p>I co-led multiple aspects of this work, building off of our prior work with ConceptFusion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">conceptgraphs</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, {Krishna Murthy} and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and {de Melo}, {Celso Miguel} and Tenenbaum, {Joshua B.} and Torralba, Antonio and Shkurti, Florian and Paull, Liam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/conceptfusion.gif" alt="A gif depicting the types of multimodal queries (text, image, audio, click) supported by ConceptFusion."></div> <div class="col-sm"></div> <div id="conceptfusion" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">ConceptFusion: Open-set Multimodal 3D Mapping</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>, <a href="https://www.alihkw.com/" target="_blank" rel="noopener noreferrer">Alihusein Kuwajerwala</a>, <a href="https://georgegu1997.github.io/" target="_blank" rel="noopener noreferrer">Qiao Gu</a>, <a href="https://scholar.google.com/citations?user=jFH3ShsAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Mohd Omama</a>, <a href="https://taochenshh.github.io/" target="_blank" rel="noopener noreferrer">Tao Chen</a>, <a href="https://people.csail.mit.edu/lishuang/" target="_blank" rel="noopener noreferrer">Shuang Li</a>, <a href="https://epiception.github.io/" target="_blank" rel="noopener noreferrer">Ganesh Iyer</a>, <a href="https://saryazdi.github.io/" target="_blank" rel="noopener noreferrer">Soroush Saryazdi</a>, <a href="https://nik-v9.github.io/" target="_blank" rel="noopener noreferrer">Nikhil Keetha</a>, <a href="https://ayushtewari.com/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>, <a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="noopener noreferrer">Joshua B. Tenenbaum</a>, <a href="https://celsodemelo.net/" target="_blank" rel="noopener noreferrer">Celso Miguel de Melo</a>, <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna</a>, <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a>, <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a>, and <a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a> </div> <div class="periodical"> <em>RSS</em> 2023 </div> <div class="links"> <a href="https://concept-fusion.github.io/assets/pdf/2023-ConceptFusion.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://concept-fusion.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/concept-fusion/concept-fusion" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=rkXgws8fiDs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason bout a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40 percent margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.</p> </div> <div class="contrib hidden"> <p>I conceived the idea and led the project. I also wrote much of the code and the paper. I curated and annotated the UnCoCo dataset and helped with the tabletop robot experiments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">conceptfusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ConceptFusion: Open-set Multimodal 3D Mapping}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, {Joshua B.} and {de Melo}, {Celso Miguel} and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RSS}</span><span class="p">,</span>
  <span class="na">dataset</span> <span class="err">and</span> <span class="err">helped</span> <span class="err">with</span> <span class="err">the</span> <span class="err">tabletop</span> <span class="err">robot</span> <span class="err">experiments.},</span>
  <span class="err">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/gradsim.png" alt="A few physical systems implemented in gradSim."></div> <div class="col-sm"></div> <div id="gradsim" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">gradSim: Differentiable simulation for system identification and visuomotor control</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>*, <a href="http://blog.mmacklin.com/about/" target="_blank" rel="noopener noreferrer">Miles Macklin</a>*, <a href="https://fgolemo.github.io/" target="_blank" rel="noopener noreferrer">Florian Golemo</a>, <a href="https://voletiv.github.io/" target="_blank" rel="noopener noreferrer">Vikram Voleti</a>, <a href="https://lindapetrini.github.io/" target="_blank" rel="noopener noreferrer">Linda Petrini</a>, <a href="http://martincsweiss.com/" target="_blank" rel="noopener noreferrer">Martin Weiss</a>, <a href="http://breandan.net/" target="_blank" rel="noopener noreferrer">Breandan Considine</a>, Jerome Parent-Levesque, <a href="https://kevincxie.github.io/" target="_blank" rel="noopener noreferrer">Kevin Xie</a>, <a href="https://iphys.wordpress.com/" target="_blank" rel="noopener noreferrer">Kenny Erleben</a>, <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a>, <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a>, <a href="https://www.cim.mcgill.ca/~derek/" target="_blank" rel="noopener noreferrer">Derek Nowrouzezahrai</a>, and <a href="https://www.cs.toronto.edu/~fidler/" target="_blank" rel="noopener noreferrer">Sanja Fidler</a> </div> <div class="periodical"> <em>ICLR</em> 2021 </div> <div class="links"> <a href="https://openreview.net/pdf?id=c_E8kFWfhp0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://gradsim.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/gradsim/gradsim" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/YuVdk1b0TVw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>In this paper, we tackle the problem of estimating object physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current best solutions to the problem require precise 3D labels which are labor intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. In this work we present gradSim, a framework that overcomes the dependence on 3D supervision by combining differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This unique combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Furthermore, our unified computation graph across dynamics and rendering engines enables the learning of challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to/better than techniques that require precise 3D labels.</p> </div> <div class="tldr hidden"> <p>Differentiable models of time-varying dynamics and image formation pipelines result in highly accurate physical parameter estimation from video and visuomotor control.</p> </div> <div class="contrib hidden"> <p>This idea was jointly conceived in a meeting which included me, Derek, Breandan, Martin, Bhairav Mehta, and Maxime Chevalier-Boisvert. Martin prototyped an initial differentiable billiards engine. I implemented the first rigid-body engine, integrated it with a differentiable renderer, and setup sys-id experiments. Miles and I then joined forces, with him focusing on the physics engine and me focusing on the physics + rendering combination and overall systems integration. I ran all of the experiments for this paper. Florian (Golemo) and Vikram created the datasets, designed experiments, and also helped with code and the manuscript. All authors participated in writing the manuscript and the author response phase. Florian (Shkurti), Derek, and Sanja nearly equally co-advised on this effort.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gradsim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{gradSim: Differentiable simulation for system identification and visuomotor control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Macklin, Miles and Golemo, Florian and Voleti, Vikram and Petrini, Linda and Weiss, Martin and Considine, Breandan and Parent-Levesque, Jerome and Xie, Kevin and Erleben, Kenny and Paull, Liam and Shkurti, Florian and Nowrouzezahrai, Derek and Fidler, Sanja}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/gradslam.png" alt="An overview of the various components of gradSLAM"></div> <div class="col-sm"></div> <div id="gradslam" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">gradSLAM: Dense SLAM meets automatic differentiation</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>, <a href="https://epiception.github.io/" target="_blank" rel="noopener noreferrer">Ganesh Iyer</a>, and <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a> </div> <div class="periodical"> <em>ICRA</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/pdf/1910.10672.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://gradslam.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/gradslam/gradslam" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=2ygtSJTmo08" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>The question of “representation” is central in the context of dense simultaneous localization and mapping (SLAM). Newer learning-based approaches have the potential to leverage data or task performance to directly inform the choice of representation. However, learning representations for SLAM has been an open question, because traditional SLAM systems are not end-to-end differentiable. In this work, we present gradSLAM, a differentiable computational graph take on SLAM. Leveraging the automatic differentiation capabilities of computational graphs, gradSLAM enables the design of SLAM systems that allow for gradient-based learning across each of their components, or the system as a whole. This is achieved by creating differentiable alternatives for each non-differentiable component in a typical dense SLAM system. Specifically, we demonstrate how to design differentiable trust-region optimizers, surface measurement and fusion schemes, as well as differentiate over rays, without sacrificing performance. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM.</p> </div> <div class="tldr hidden"> <p>We present end-to-end differentiable dense SLAM systems that open up new possibilites for integrating deep learning and SLAM</p> </div> <div class="contrib hidden"> <p>I came up with the idea and led this work. Ganesh Iyer was instrumental with implementing the surfel and point-based fusion pipelines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gradslam</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{gradSLAM: Dense SLAM meets automatic differentiation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Iyer, Ganesh and Paull, Liam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> </div> <div class="contact-note"> The best way to reach me is over email (or sometimes a Twitter DM). </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Krishna Murthy Jatavallabhula. Adopted and tweaked the <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: June 14, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KVVBP90SEH"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-KVVBP90SEH");</script> </body> </html>