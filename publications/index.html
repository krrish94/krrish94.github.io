<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Krishna Murthy Jatavallabhula</title> <meta name="author" content="Krishna Murthy Jatavallabhula"/> <meta name="description" content="Krishna Murthy - Research scientist, Meta "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://krrish94.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://krrish94.github.io/"><span class="font-weight-bold">Krishna</span> Murthy Jatavallabhula</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <p>For a list of my <em>featured</em> publications, please scroll down the <a href="/">home</a> page. For an exhaustive list that includes workshop papers, please see my CV.</p> <p><sup>* indicates that authors contributed equally</sup></p> <div class="publications"> <div class="title"><p style="background-color: lightyellow; color: red">Indicates featured/representative publications</p></div> <h2 class="year">2025</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/locate3d.png" alt="An image showing the output text labels from the locate 3d model."></div> <div class="col-sm"></div> <div id="locate3d" class="col-sm-8"> <div class="title">Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D</div> <div class="author"> Sergio Arnaud, Paul McVay, Ada Martin, Arjun Majumdar,  <em>Krishna Murthy Jatavallabhula</em>, Phillip Thomas, Ruslan Partsey, Daniel Dugas, Abha Gejji, Alexander Sax, Vincent-Pierre Berges, Mikael Henaff, Ayush Jain, Ang Cao, Ishita Prasad, Mrinal Kalakrishnan, Michael Rabbat, Nicolas Ballas, Mido Assran, Oleksandr Maksymets, Aravind Rajeswaran, and Franziska Meier </div> <div class="periodical"> <em>ICML</em> 2025 </div> <div class="links"> <a href="https://locate3d.atmeta.com/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like "the small coffee table between the sofa and the lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.</p> </div> <div class="contrib hidden"> <p>I worked on the data preprocessing stage of Locate 3D.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">locate3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arnaud, Sergio and McVay, Paul and Martin, Ada and Majumdar, Arjun and Jatavallabhula, {Krishna Murthy} and Thomas, Phillip and Partsey, Ruslan and Dugas, Daniel and Gejji, Abha and Sax, Alexander and Berges, Vincent-Pierre and Henaff, Mikael and Jain, Ayush and Cao, Ang and Prasad, Ishita and Kalakrishnan, Mrinal and Rabbat, Michael and Ballas, Nicolas and Assran, Mido and Maksymets, Oleksandr and Rajeswaran, Aravind and Meier, Franziska}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/gs_vmpc.gif" alt="A gif showing a robot separating granular media into two piles."></div> <div class="col-sm"></div> <div id="gs-mpc" class="col-sm-8"> <div class="title">Gaussian Splatting Visual MPC for Granular Media Manipulation</div> <div class="author"> Wei-Cheng Tseng, Ellina Zhang,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a> </div> <div class="periodical"> <em>ICRA</em> 2025 </div> <div class="links"> <a href="https://arxiv.org/pdf/2410.09740.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://weichengtseng.github.io/gs-granular-mani/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Recent advancements in learned 3D representations have enabled significant progress in solving complex robotic manipulation tasks, particularly for rigid-body objects. However, manipulating granular materials such as beans, nuts, and rice, remains challenging due to the intricate physics of particle interactions, high-dimensional and partially observable state, inability to visually track individual particles in a pile, and the computational demands of accurate dynamics prediction. Current deep latent dynamics models often struggle to generalize in granular material manipulation due to a lack of inductive biases. In this work, we propose a novel approach that learns a visual dynamics model over Gaussian splatting representations of scenes and leverages this model for manipulating granular media via Model-Predictive Control. Our method enables efficient optimization for complex manipulation tasks on piles of granular media. We evaluate our approach in both simulated and real-world settings, demonstrating its ability to solve unseen planning tasks and generalize to new environments in a zero-shot transfer. We also show significant prediction and manipulation performance improvements compared to existing granular media manipulation methods.</p> </div> <div class="contrib hidden"> <p>I co-advised Wei-Cheng on this project.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gs-mpc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gaussian Splatting Visual MPC for Granular Media Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tseng, Wei-Cheng and Zhang, Ellina and Jatavallabhula, {Krishna Murthy} and Shkurti, Florian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2024</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/splatam.gif" alt="A gif depicting the 3D mapping process implemented as part of SplaTAM."></div> <div class="col-sm"></div> <div id="splatam" class="col-sm-8"> <div class="title">SplaTAM:Splat, Track, and Map 3D Gaussians for Dense RGB-D SLAM</div> <div class="author"> <a href="https://nik-v9.github.io/" target="_blank" rel="noopener noreferrer">Nikhil Keetha</a>, <a href="https://jaykarhade.github.io/" target="_blank" rel="noopener noreferrer">Jay Karhade</a>,  <em>Krishna Murthy Jatavallabhula</em>, Gengshan Yang, <a href="https://theairlab.org/team/sebastian/" target="_blank" rel="noopener noreferrer">Sebastian Scherer</a>, Deva Ramanan, and Jonathon Luiten </div> <div class="periodical"> <em>CVPR</em> 2024 </div> <div class="links"> <a href="https://spla-tam.github.io/assets/SplaTAM.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://spla-tam.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/spla-tam/SplaTAM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/jWLI-OFp3qU" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by a 3D Gaussian Splatting radiance field can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. In particular, we employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments on simulated and real-world data show that SplaTAM achieves up to 2 X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches.</p> </div> <div class="contrib hidden"> <p>This project was spearheaded by Nikhil and Jonathon. I interfaced Gaussian Splatting with GradSLAM, and provided SLAM-specific guidance over the course of the project.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">splatam</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SplaTAM:Splat, Track, and Map 3D Gaussians for Dense RGB-D SLAM}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Keetha, Nikhil and Karhade, Jay and Jatavallabhula, {Krishna Murthy} and Yang, Gengshan and Scherer, Sebastian and Ramanan, Deva and Luiten, Jonathon}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/conceptgraphs.gif" alt="A gif depicting the 3D mapping process implemented as part of ConceptGraphs."></div> <div class="col-sm"></div> <div id="conceptgraphs" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</p></div> <div class="author"> <a href="https://georgegu1997.github.io/" target="_blank" rel="noopener noreferrer">Qiao Gu</a>*, <a href="https://www.alihkw.com/" target="_blank" rel="noopener noreferrer">Alihusein Kuwajerwala</a>*, <a href="https://sachamorin.github.io/" target="_blank" rel="noopener noreferrer">Sacha Morin</a>*,  <em>Krishna Murthy Jatavallabhula</em>*, <a href="https://bipashasen.github.io/" target="_blank" rel="noopener noreferrer">Bipasha Sen</a>, <a href="https://skymanaditya1.github.io/" target="_blank" rel="noopener noreferrer">Aditya Agarwal</a>, <a href="https://www.jhuapl.edu/work/our-organization/research-and-exploratory-development/red-staff-directory/corban-rivera" target="_blank" rel="noopener noreferrer">Corban Rivera</a>, <a href="https://scholar.google.com/citations?user=92bmh84AAAAJ" target="_blank" rel="noopener noreferrer">William Paul</a>, <a href="https://mila.quebec/en/person/kirsty-ellis/" target="_blank" rel="noopener noreferrer">Kirsty Ellis</a>, <a href="https://engineering.jhu.edu/faculty/rama-chellappa/" target="_blank" rel="noopener noreferrer">Rama Chellappa</a>, <a href="https://people.csail.mit.edu/ganchuang/" target="_blank" rel="noopener noreferrer">Chuang Gan</a>, <a href="https://celsodemelo.net/" target="_blank" rel="noopener noreferrer">Celso Miguel de Melo</a>, <a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="noopener noreferrer">Joshua B. Tenenbaum</a>, <a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a>, <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a>, and <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a> </div> <div class="periodical"> <em>ICRA</em> 2024 </div> <div class="links"> <a href="https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://concept-graphs.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/concept-graphs/concept-graphs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/mRhNkQwRYnc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts.</p> </div> <div class="contrib hidden"> <p>I co-led multiple aspects of this work, building off of our prior work with ConceptFusion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">conceptgraphs</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, {Krishna Murthy} and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and {de Melo}, {Celso Miguel} and Tenenbaum, {Joshua B.} and Torralba, Antonio and Shkurti, Florian and Paull, Liam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iml.png" alt="An example output instance localization image of the iML system implemented on a living room scene from the Replica dataset."></div> <div class="col-sm"></div> <div id="iml" class="col-sm-8"> <div class="title">iML: Efficient 3D Instance Mapping and Localization</div> <div class="author"> George Tang, <a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a>, and <em>Krishna Murthy Jatavallabhula</em> </div> <div class="periodical"> <em>ICRA</em> 2024 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">iml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{iML: Efficient 3D Instance Mapping and Localization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, George and Torralba, Antonio and Jatavallabhula, {Krishna Murthy}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/stable-placement.gif" alt="A robot gently stacking two light-weight objects using tactile sensing and our proposed method."></div> <div class="col-sm"></div> <div id="tactile-placement" class="col-sm-8"> <div class="title">Tactile Estimation of Extrinsic Contact Patch for Stable Placement</div> <div class="author"> <a href="https://keiohta.github.io/" target="_blank" rel="noopener noreferrer">Kei Ota</a>, <a href="https://www.merl.com/people/jha" target="_blank" rel="noopener noreferrer">Devesh Jha</a>,  <em>Krishna Murthy Jatavallabhula</em>, <a href="https://kanezaki.github.io/" target="_blank" rel="noopener noreferrer">Asako Kanezaki</a>, and <a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="noopener noreferrer">Joshua B. Tenenbaum</a> </div> <div class="periodical"> <em>ICRA</em> 2024 </div> <div class="links"> <a href="https://arxiv.org/pdf/2309.14552.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.</p> </div> <div class="contrib hidden"> <p>Kei and Devesh did much of the work. I was consulted for advice with tactile sensors and problem framing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tactile-placement</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tactile Estimation of Extrinsic Contact Patch for Stable Placement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ota, Kei and Jha, Devesh and Jatavallabhula, {Krishna Murthy} and Kanezaki, Asako and Tenenbaum, {Joshua B.}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/talk2bev.gif" alt="A gif depicting a conversation with the Talk2BEV system. The user asks how they should move 20 m forward, and the system responds by saying there is a vehicle in front that we may need to pass by changing to the other lane."></div> <div class="col-sm"></div> <div id="talk2bev" class="col-sm-8"> <div class="title">Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving</div> <div class="author"> <a href="https://vikr-182.github.io/" target="_blank" rel="noopener noreferrer">Vikrant Dewangan</a>, Tushar Choudhary*, <a href="https://scholar.google.com/citations?user=ZER2BeIAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Shivam Chandhok</a>*, Shubham Priyadarshan, Anushka Jain, <a href="https://sites.google.com/view/akslab" target="_blank" rel="noopener noreferrer">Arun Singh</a>, <a href="https://siddharthsrivastava.github.io/" target="_blank" rel="noopener noreferrer">Siddharth Srivastava</a>,  <em>Krishna Murthy Jatavallabhula</em>*, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna</a> </div> <div class="periodical"> <em>ICRA</em> 2024 </div> <div class="links"> <a href="https://llmbev.github.io/talk2bev/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/llmbev/talk2bev" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=TMht-8SGJ0I" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>This work introduces Talk2BEV, a large vision-language model (LVLM) interface for bird’s-eye view (BEV) maps commonly used in autonomous driving. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV eliminates the need for BEV-specific training, relying instead on performant pre-trained LVLMs. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV n a large number of scene understanding tasks that rely on both the ability to interpret freefrom natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset.</p> </div> <div class="contrib hidden"> <p>I conceived the initial idea and played a lead-advisor role on the project.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">talk2bev</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dewangan, Vikrant and Choudhary, Tushar and Chandhok, Shivam and Priyadarshan, Shubham and Jain, Anushka and Singh, Arun and Srivastava, Siddharth and Jatavallabhula, {Krishna Murthy} and Krishna, Madhava}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/task-anticipation.gif" alt="A gif depicting the task anticipation and execution process on a simulated household agent in the VirtualHome environment."></div> <div class="col-sm"></div> <div id="task-anticipation" class="col-sm-8"> <div class="title">Anticipate &amp; Act: Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments</div> <div class="author"> Raghav Arora, Shivam Singh, Karthik Swaminathan, Ahana Datta, Snehasis Banerjee, <a href="https://sites.google.com/view/brojeshwar/home" target="_blank" rel="noopener noreferrer">Brojeshwar Bhowmick</a>,  <em>Krishna Murthy Jatavallabhula</em>, Mohan Sridharan, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna</a> </div> <div class="periodical"> <em>ICRA</em> 2024 </div> <div class="links"> <a href="https://raraghavarora.github.io/ahsoka/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Assistive agents performing household tasks such as making the bed, preparing coffee, or cooking breakfast, often consider one task at a time by computing a plan of actions that accomplishes this task. The agents can be more efficient by anticipating upcoming tasks, and computing and executing an action sequence that jointly achieves these tasks. State of the art methods for task anticipating use data-driven deep network architectures and Large Language Models (LLMs) for task estimation but they do so at the level of high-level tasks and/or require a large number of training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as joint goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework’s capabilities in realistic simulated scenarios in the VirtualHome environment and demonstrate a 31 percent reduction in the execution time in comparison with a system that does not consider upcoming tasks.</p> </div> <div class="contrib hidden"> <p>I closely mentored the students on this project, including conceptualizing the broad theme and advising on experiment design. Raghav and Shivam did nearly all of the work. Mohan wrote a bulk of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">task-anticipation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Anticipate &amp; Act: Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arora, Raghav and Singh, Shivam and Swaminathan, Karthik and Datta, Ahana and Banerjee, Snehasis and Bhowmick, Brojeshwar and Jatavallabhula, {Krishna Murthy} and Sridharan, Mohan and Krishna, Madhava}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/follow-anything.gif" alt="A gif depicting the Follow Anything system tracking and following an object in real-time."></div> <div class="col-sm"></div> <div id="fam" class="col-sm-8"> <div class="title">Follow Anything: Open-set detection, tracking, and following in real-time</div> <div class="author"> Alaa Maalouf, Ninad Jadhav,  <em>Krishna Murthy Jatavallabhula</em>, Makram Chahine, Daniel M Vogt, Robert J Wood, <a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a>, and Daniela Rus </div> <div class="periodical"> <em>Robotics and Automation Letters</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed “follow anything” (FAn), is an open-vocabulary and multimodal model – it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second.</p> </div> <div class="contrib hidden"> <p>Alaa and Ninad did most of the work. I was consulted primarily for systems building advice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fam</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Follow Anything: Open-set detection, tracking, and following in real-time}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Maalouf, Alaa and Jadhav, Ninad and Jatavallabhula, {Krishna Murthy} and Chahine, Makram and Vogt, {Daniel M} and Wood, {Robert J} and Torralba, Antonio and Rus, Daniela}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics and Automation Letters}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/altpilot.gif" alt="Our system, ALT-Pilot, autonomously driving to a language-specified destination."></div> <div class="col-sm"></div> <div id="altpilot" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">ALT-Pilot: Autonomous navigation with Language augmented Topometric maps</p></div> <div class="author"> <a href="https://scholar.google.com/citations?user=jFH3ShsAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Mohd Omama</a>, <a href="https://scholar.google.com/citations?user=1K8EuVEAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Pranav Inani</a>, Pranjal Paul, Sarat Chandra Yellapragada,  <em>Krishna Murthy Jatavallabhula</em>*, <a href="https://utaustin-swarmlab.github.io/people/sandeep_chinchali/index.html" target="_blank" rel="noopener noreferrer">Sandeep Chinchali</a>*, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna</a> </div> <div class="periodical"> <em>preprint</em> 2023 </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.02324.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://navigate-anywhere.github.io/ALT-Pilot/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/sw7MuZs1q-0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>We present an autonomous navigation system that operates without assuming HD LiDAR maps of the environment. Our system, ALT-Pilot, relies only on publicly available road network information and a sparse (and noisy) set of crowdsourced language landmarks. With the help of onboard sensors and a language-augmented topometric map, ALT-Pilot autonomously pilots the vehicle to any destination on the road network. We achieve this by leveraging vision- language models pre-trained on web-scale data to identify potential landmarks in a scene, incorporating vision-language features into the recursive Bayesian state estimation stack to generate global (route) plans, and a reactive trajectory planner and controller operating in the vehicle frame. We implement and evaluate ALT-Pilot in simulation and on a real, full-scale autonomous vehicle and report improvements over state-of- the-art topometric navigation systems by a factor of 3x on localization accuracy and 5x on goal reachability.</p> </div> <div class="contrib hidden"> <p>Idea jointly conceived by Omama and me. I established project directions/goals; closely mentored Omama, Pranjal, and Pranav. Omama did bulk of the implementation, with significant contributions also from Pranjal and Pranav.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {ALT-Pilot: Autonomous navigation with Language augmented Topometric maps},</span>
  <span class="c">author = {Omama, Mohd and Inani, Pranav and Paul, Pranjal and Yellapragada, {Sarat Chandra} and Jatavallabhula, {Krishna Murthy} and Chinchali, Sandeep and Krishna, Madhava},</span>
  <span class="c">year = {2023},</span>
  <span class="c">booktitle = {preprint},</span>
  <span class="c">featured = {true},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/diff-visual-computing-review.png" alt=""></div> <div class="col-sm"></div> <div id="diff-visual-computing-review" class="col-sm-8"> <div class="title">Differentiable Visual Computing for Inverse Problems and Machine Learning</div> <div class="author"> <a href="http://www.andrewspielberg.com/" target="_blank" rel="noopener noreferrer">Andrew Spielberg</a>, <a href="https://www.cl.cam.ac.uk/~aco41/" target="_blank" rel="noopener noreferrer">Cengiz Oztireli</a>, <a href="https://www.cim.mcgill.ca/~derek/" target="_blank" rel="noopener noreferrer">Derek Nowrouzezahrai</a>, <a href="https://www.cl.cam.ac.uk/~fz261/" target="_blank" rel="noopener noreferrer">Fangcheng Zhong</a>, <a href="http://www.krematas.com/" target="_blank" rel="noopener noreferrer">Konstantinos Rematas</a>,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://cseweb.ucsd.edu/~tzli/" target="_blank" rel="noopener noreferrer">Tzu-Mao Li</a> </div> <div class="periodical"> <em>Nature Machine Intelligence</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Modern 3D computer graphics technologies are able to reproduce the dynamics and appearance of real world environments and phenomena, building atop theoretical models in applied mathematics, statistics, and physics. These methods are applied in architectural design and visualization, biological imaging, and visual effects. Differentiable methods, instead, aim to determine how graphics outputs (i.e., the real world dynamics or appearance) change when the environment changes. We survey this growing body of work and propose a holistic and unified differentiable visual computing pipeline. Differentiable visual computing can be leveraged to efficiently solve otherwise intractable problems in physical inference, optimal control, object detection and scene understanding, computational design, manufacturing, autonomous vehicles, and robotics. Any application that can benefit from an understanding of the underlying dynamics of the real world stands to benefit significantly from a differentiable graphics treatment. We draw parallels between the well-established computer graphics pipeline and a unified differentiable graphics pipeline, targeting consumers, practitioners and researchers. The breadth of fields that these pipelines draws upon — and are of interest to — includes the physical sciences, data sciences, vision and graphics, machine learning, and adjacent mathematical and computing communities.</p> </div> <div class="contrib hidden"> <p>Andy led this work. Derek was the lead PI.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">diff-visual-computing-review</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Differentiable Visual Computing for Inverse Problems and Machine Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Spielberg, Andrew and Oztireli, Cengiz and Nowrouzezahrai, Derek and Zhong, Fangcheng and Rematas, Konstantinos and Jatavallabhula, {Krishna Murthy} and Li, Tzu-Mao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Nature Machine Intelligence}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/anyloc.gif" alt="A gif depicting the various kinds of scenes (outdoors, indoors, underwater, aerial) that AnyLoc was deployed in."></div> <div class="col-sm"></div> <div id="anyloc" class="col-sm-8"> <div class="title">AnyLoc: Towards Universal Visual Place Recognition</div> <div class="author"> <a href="https://nik-v9.github.io/" target="_blank" rel="noopener noreferrer">Nikhil Keetha</a>*, <a href="https://theprojectsguy.github.io/" target="_blank" rel="noopener noreferrer">Avneesh Mishra</a>*, <a href="https://jaykarhade.github.io/" target="_blank" rel="noopener noreferrer">Jay Karhade</a>*,  <em>Krishna Murthy Jatavallabhula</em>, <a href="https://theairlab.org/team/sebastian/" target="_blank" rel="noopener noreferrer">Sebastian Scherer</a>, <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna</a>, and <a href="https://researchers.adelaide.edu.au/profile/sourav.garg" target="_blank" rel="noopener noreferrer">Sourav Garg</a> </div> <div class="periodical"> <em>Robotics and Automation Letters (and ICRA 2024)</em> 2023 </div> <div class="links"> <a href="https://arxiv.org/pdf/2308.00688.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/AnyLoc/AnyLoc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/ITo8rMInatk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>VPR is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust realworld deployment. In this work, we develop a universal solution to VPR – a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or finetuning. We demonstrate that general-purpose feature representations derived from off-theshelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4× significantly higher performance than existing approaches. We further obtain a 6 percent improvement in performance by characterizing the semantic properties of these features, uncovering unique domains which encapsulate datasets from similar environments. Our detailed experiments and analysis lay a foundation for building VPR solutions that may be deployed anywhere, anytime, and across anyview.</p> </div> <div class="contrib hidden"> <p>Nikhil, Avneesh, and Jay equally led this work. Sourav and I closely mentored them on this work (Sourav more closely than me).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">anyloc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AnyLoc: Towards Universal Visual Place Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Keetha, Nikhil and Mishra, Avneesh and Karhade, Jay and Jatavallabhula, {Krishna Murthy} and Scherer, Sebastian and Krishna, Madhava and Garg, Sourav}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics and Automation Letters (and ICRA 2024)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/conceptfusion.gif" alt="A gif depicting the types of multimodal queries (text, image, audio, click) supported by ConceptFusion."></div> <div class="col-sm"></div> <div id="conceptfusion" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">ConceptFusion: Open-set Multimodal 3D Mapping</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>, <a href="https://www.alihkw.com/" target="_blank" rel="noopener noreferrer">Alihusein Kuwajerwala</a>, <a href="https://georgegu1997.github.io/" target="_blank" rel="noopener noreferrer">Qiao Gu</a>, <a href="https://scholar.google.com/citations?user=jFH3ShsAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Mohd Omama</a>, <a href="https://taochenshh.github.io/" target="_blank" rel="noopener noreferrer">Tao Chen</a>, <a href="https://people.csail.mit.edu/lishuang/" target="_blank" rel="noopener noreferrer">Shuang Li</a>, <a href="https://epiception.github.io/" target="_blank" rel="noopener noreferrer">Ganesh Iyer</a>, <a href="https://saryazdi.github.io/" target="_blank" rel="noopener noreferrer">Soroush Saryazdi</a>, <a href="https://nik-v9.github.io/" target="_blank" rel="noopener noreferrer">Nikhil Keetha</a>, <a href="https://ayushtewari.com/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>, <a href="http://web.mit.edu/cocosci/josh.html" target="_blank" rel="noopener noreferrer">Joshua B. Tenenbaum</a>, <a href="https://celsodemelo.net/" target="_blank" rel="noopener noreferrer">Celso Miguel de Melo</a>, <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna</a>, <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a>, <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a>, and <a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank" rel="noopener noreferrer">Antonio Torralba</a> </div> <div class="periodical"> <em>RSS</em> 2023 </div> <div class="links"> <a href="https://concept-fusion.github.io/assets/pdf/2023-ConceptFusion.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://concept-fusion.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/concept-fusion/concept-fusion" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=rkXgws8fiDs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason bout a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40 percent margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.</p> </div> <div class="contrib hidden"> <p>I conceived the idea and led the project. I also wrote much of the code and the paper. I curated and annotated the UnCoCo dataset and helped with the tabletop robot experiments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">conceptfusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ConceptFusion: Open-set Multimodal 3D Mapping}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, {Joshua B.} and {de Melo}, {Celso Miguel} and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RSS}</span><span class="p">,</span>
  <span class="na">dataset</span> <span class="err">and</span> <span class="err">helped</span> <span class="err">with</span> <span class="err">the</span> <span class="err">tabletop</span> <span class="err">robot</span> <span class="err">experiments.},</span>
  <span class="err">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/pacnerf.gif" alt="A demo from PAC-NeRF – estimating the geometry and physical properties of a deformable torus"></div> <div class="col-sm"></div> <div id="pacnerf" class="col-sm-8"> <div class="title">PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for Geometry-Agnostic System Identification</div> <div class="author"> <a href="https://xuan-li.github.io/" target="_blank" rel="noopener noreferrer">Xuan Li</a>, <a href="https://ylqiao.net/" target="_blank" rel="noopener noreferrer">Yi-Ling Qiao</a>, <a href="https://peterchencyc.com/" target="_blank" rel="noopener noreferrer">Peter Yichen Chen</a>,  <em>Krishna Murthy Jatavallabhula</em>, <a href="https://www.cs.umd.edu/~lin/" target="_blank" rel="noopener noreferrer">Ming Lin</a>, <a href="https://www.math.ucla.edu/~cffjiang/" target="_blank" rel="noopener noreferrer">Chenfanfu Jiang</a>, and <a href="https://people.csail.mit.edu/ganchuang/" target="_blank" rel="noopener noreferrer">Chuang Gan</a> </div> <div class="periodical"> <em>ICLR</em> 2023 </div> <font color="red"><b> Notable top 25 (top 25 percent of accepted submissions) </b></font> <div class="links"> <a href="https://openreview.net/pdf?id=tVkrbkz42vc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://sites.google.com/view/pac-nerf/overview?authuser=0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/xuan-li/PAC-NeRF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Existing approaches to system identification (estimating the physical parameters of an object) from videos assume known object geometries. This precludes their applicability in a vast majority of scenes where object geometries are complex or unknown. In this work, we aim to identify parameters characterizing a physical system from a set of multi-view videos without any assumption on object geometry or topology. To this end, we propose "Physics Augmented Continuum Neural Radiance Fields" (PAC-NeRF), to estimate both the unknown geometry and physical parameters of highly dynamic objects from multi-view videos. We design PAC-NeRF to only ever produce physically plausible states by enforcing the neural radiance field to follow the conservation laws of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian representation of the neural radiance field, i.e., we use the Eulerian grid representation for NeRF density and color fields, while advecting the neural radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian representation seamlessly blends efficient neural rendering with the material point method (MPM) for robust differentiable physics simulation. We validate the effectiveness of our proposed framework on geometry and physical parameter estimation over a vast range of materials, including elastic bodies, plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate significant performance gain on most tasks.</p> </div> <div class="tldr hidden"> <p>We integrate a differentiable physics engine with neural radiance fields for simulatneous estimation of geometry and physical properties from multi-view videos</p> </div> <div class="contrib hidden"> <p>Work primarily done by Xuan when interning with Chuang, who was the lead PI. Chuang came up with the central pitch for this work. I helped design the real-world experiments. My biggest contribution to this project was perhaps that I wrote up bulk of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pacnerf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for Geometry-Agnostic System Identification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Xuan and Qiao, Yi-Ling and Chen, {Peter Yichen} and Jatavallabhula, {Krishna Murthy} and Lin, Ming and Jiang, Chenfanfu and Gan, Chuang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">recognition</span> <span class="p">=</span> <span class="s">{Notable top 25 (top 25 percent of accepted submissions)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/muhle2023cvpr.gif" alt="A gif depicting the uncertainty estimation results over an autonomous driving sequence from KITTI."></div> <div class="col-sm"></div> <div id="muhle2023cvpr" class="col-sm-8"> <div class="title">Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares</div> <div class="author"> <a href="https://dominikmuhle.github.io/" target="_blank" rel="noopener noreferrer">Dominik Muhle</a>, <a href="https://lukaskoestler.com/" target="_blank" rel="noopener noreferrer">Lukas Koestler</a>,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://cvg.cit.tum.de/members/cremers" target="_blank" rel="noopener noreferrer">Daniel Cremers</a> </div> <div class="periodical"> <em>CVPR</em> 2023 </div> <div class="links"> <a href="https://dominikmuhle.github.io/dnls_covs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>We propose a differentiable nonlinear least squares framework to account for uncertainty in relative pose estimation from feature correspondences. Specifically, we introduce a symmetric version of the probabilistic normal epipolar constraint, and an approach to estimate the covariance of feature positions by differentiating through the camera pose estimation procedure. We evaluate our approach on synthetic, as well as the KITTI and EuRoC real-world datasets. On the synthetic dataset, we confirm that our learned covariances accurately approximate the true noise distribution. In real world experiments, we find that our approach consistently outperforms state-of-the-art non-probabilistic and probabilistic approaches, regardless of the feature extraction algorithm of choice.</p> </div> <div class="contrib hidden"> <p>Work led entirely by Dominik, and Lukas assisted with the formulation of several technical details. Daniel and I offered overarching guidance and support.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">muhle2023cvpr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Muhle, Dominik and Koestler, Lukas and Jatavallabhula, {Krishna Murthy} and Cremers, Daniel}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/bom.png" alt="A pipeline figure for the Bayesian Object Models work"></div> <div class="col-sm"></div> <div id="bom" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">Bayesian Object Models for Robotic Interaction with Differentiable Probabilistic Programming</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>, <a href="http://blog.mmacklin.com/about/" target="_blank" rel="noopener noreferrer">Miles Macklin</a>, <a href="https://homes.cs.washington.edu/~fox/" target="_blank" rel="noopener noreferrer">Dieter Fox</a>, Animesh Garg, and <a href="https://fabioramos.github.io/Home.html" target="_blank" rel="noopener noreferrer">Fabio Ramos</a> </div> <div class="periodical"> <em>CoRL</em> 2022 </div> <div class="links"> <a href="https://openreview.net/pdf?id=QSUsBMuw0uV" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>A hallmark of human intelligence is the ability to build rich mental models of previously unseen objects from very few interactions. To achieve true, continuous autonomy, robots too must possess this ability. Importantly, to integrate with the probabilistic robotics software stack, such models must encapsulate the uncertainty (resulting from noisy dynamics and observation models) in a prescriptive manner. We present Bayesian Object Models (BOMs): generative (probabilistic) models that encode both the structural and kinodynamic attributes of an object. BOMs are implemented in the form of a differentiable probabilistic program that models latent scene structure, object dynamics, and observation models. This allows for efficient and automated Bayesian inference – samples (object trajectories) drawn from the BOM are compared with a small set of real-world observations and used to compute a likelihood function. Our model comprises a differentiable tree structure sampler and a differentiable physics engine, enabling gradient computation through this likelihood function. This enables gradient-based Bayesian inference to efficiently update the distributional parameters of our model. BOMs outperform several recent approaches, including differentiable physics-based, gradient-free, and neural inference schemes.</p> </div> <div class="tldr hidden"> <p>We build a differentiable probabilistic physics engine that can estimate posterior distributions over object articulation chains and physical properties</p> </div> <div class="contrib hidden"> <p>I led this work during my (virtual) internship at NVIDIA in summer 2021</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bom</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bayesian Object Models for Robotic Interaction with Differentiable Probabilistic Programming}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Macklin, Miles and Fox, Dieter and Garg, Animesh and Ramos, Fabio}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CoRL}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/bodiffsim.jpg" alt="A didactic figure explaining our core algorithm, and showing some environments we use for evaluation."></div> <div class="col-sm"></div> <div id="bodiffsim" class="col-sm-8"> <div class="title">Rethinking Optimization with Differentiable Simulation from a Global Perspective</div> <div class="author"> <a href="https://contactrika.github.io/" target="_blank" rel="noopener noreferrer">Rika Antonova</a>*, <a href="https://yjy0625.github.io/" target="_blank" rel="noopener noreferrer">Jingyun Yang</a>*,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://web.stanford.edu/~bohg/" target="_blank" rel="noopener noreferrer">Jeannette Bohg</a> </div> <div class="periodical"> <em>CoRL</em> 2022 </div> <font color="red"><b> Oral presentation (top 6.5 percent of submissions) </b></font> <div class="links"> <a href="https://arxiv.org/pdf/2207.00167.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://yjy0625.github.io/projects/globdiff/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://yjy0625.github.io/projects/globdiff/video/rethinking-diffsim.mp4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Differentiable simulation is a promising toolkit for fast gradient-based policy optimization and system identification. In this work, we study the challenges that differentiable simulation presents when it is not feasible to expect that a single descent reaches a global optimum. We analyze the optimization landscapes of diverse scenarios and find that in dynamic environments with highly deformable objects and fluids, differentiable simulators produce rugged landscapes with useful gradients. We propose a method that combines Bayesian optimization with semi-local leaps to obtain a global search method that can use gradients effectively and maintain robust performance in regions with noisy gradients. We show extensive experiments in simulation, and also validate the method in a real robot setup.</p> </div> <div class="tldr hidden"> <p>We show that differentiable simulations present difficult optimization landscapes and address this with a method that combines global and local optimization</p> </div> <div class="contrib hidden"> <p>Rika and Jingyun did nearly all of the work. I was consulted primarily for advice on differentiable simulation. I helped write a few parts of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bodiffsim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rethinking Optimization with Differentiable Simulation from a Global Perspective}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Antonova, Rika and Yang, Jingyun and Jatavallabhula, {Krishna Murthy} and Bohg, Jeannette}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CoRL}</span><span class="p">,</span>
  <span class="na">recognition</span> <span class="p">=</span> <span class="s">{Oral presentation (top 6.5 percent of submissions)}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/fcal.gif" alt="Sample object detection (and uncertainty estimates) from our approach."></div> <div class="col-sm"></div> <div id="fcal" class="col-sm-8"> <div class="title">f -Cal: Calibrated aleatoric uncertainty estimation from neural networks for robot perception</div> <div class="author"> <a href="https://dhaivat1729.github.io/" target="_blank" rel="noopener noreferrer">Dhaivat Bhatt</a>*, <a href="https://hbutsuak95.github.io/" target="_blank" rel="noopener noreferrer">Kaustubh Mani</a>*, <a href="https://dishank-b.github.io/" target="_blank" rel="noopener noreferrer">Dishank Bansal</a>, Hanju Lee,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a> </div> <div class="periodical"> <em>ICRA</em> 2022 </div> <div class="links"> <a href="https://f-cal.github.io/pdf/f-Cal-preprint.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://f-cal.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/f-cal/f_cal" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/2JVVfySNATM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>f-Cal is calibration method for probabilistic regression networks. Typical Bayesian neural networks are overconfident in their predictions. For these predictions to be used in downstream tasks, reliable and calibrated uncertainity estimates are critical. f-Cal proposes a simple loss function to remedy this; this can be employed to train any probabilistic neural regressor to produced calibrated estimates of aleatoric uncertainty.</p> </div> <div class="tldr hidden"> <p>We present a simple approach that uses a variational loss to enforce calibration in probabilistic regression networks</p> </div> <div class="contrib hidden"> <p>Dhaivat and Kaustubh contributed equally to the experiments. Dishank implemented very early prototypes. Liam came up with this idea. I mentored Dhaivat and Kaustubh closely on this work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fcal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{f -Cal: Calibrated aleatoric uncertainty estimation from neural networks for robot perception}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bhatt, Dhaivat and Mani, Kaustubh and Bansal, Dishank and Lee, Hanju and Jatavallabhula, {Krishna Murthy} and Paull, Liam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/draco.png" alt="A few representative results from our system - DRACO."></div> <div class="col-sm"></div> <div id="draco" class="col-sm-8"> <div class="title">DRACO: Weakly supervised dense reconstruction and canonicalization of objects</div> <div class="author"> <a href="https://scholar.google.com/citations?user=HAtfBjoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Rahul Sajnani</a>*, Aadil Mehdi Sanchawala*,  <em>Krishna Murthy Jatavallabhula</em>, <a href="https://cs.brown.edu/people/ssrinath" target="_blank" rel="noopener noreferrer">Srinath Sridhar</a>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>ICRA</em> 2022 </div> <div class="links"> <a href="https://aadilmehdis.github.io/DRACO-Project-Page/content/2021_ICRA_DRACO.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://aadilmehdis.github.io/DRACO-Project-Page/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/RahulSajnani/DRACO-Weakly-Supervised-Dense-Reconstruction-And-Canonicalization-of-Objects" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/jdvLaBw-pN4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>We present DRACO, a method for Dense Reconstruction And Canonicalization of Object shape from one or more RGB images. Canonical shape reconstruction; estimating 3D object shape in a coordinate space canonicalized for scale, rotation, and translation parameters—is an emerging paradigm that holds promise for a multitude of robotic applications. Prior approaches either rely on painstakingly gathered dense 3D supervision, or produce only sparse canonical representations, limiting real-world applicability. DRACO performs dense canonicalization using only weak supervision in the form of camera poses and semantic keypoints at train time. During inference, DRACO predicts dense object-centric depth maps in a canonical coordinate-space, solely using one or more RGB images of an object. Extensive experiments on canonical shape reconstruction and pose estimation show that DRACO is competitive or superior to fully-supervised methods.</p> </div> <div class="tldr hidden"> <p>We present a weakly supervised approach that reconstructs objects in a canonical coordinate space</p> </div> <div class="contrib hidden"> <p>I mentored Rahul and Aadil on this project. Srinath and Madhav were the lead PIs, and likely contributed more than me.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">draco</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DRACO: Weakly supervised dense reconstruction and canonicalization of objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sajnani, Rahul and Sanchawala, {Aadil Mehdi} and Jatavallabhula, {Krishna Murthy} and Sridhar, Srinath and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/taskography.png" alt="A sample 3D scene graph of a small house containing multiple rooms."></div> <div class="col-sm"></div> <div id="taskography" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">Taskography: Evaluating robot task planning over large 3D scene graphs</p></div> <div class="author"> <a href="https://www.chrisagia.com/" target="_blank" rel="noopener noreferrer">Chris Agia</a>*,  <em>Krishna Murthy Jatavallabhula</em>*, Mohamed Khodeir, Ondrej Miksik, <a href="http://vibhavvineet.info/" target="_blank" rel="noopener noreferrer">Vibhav Vineet</a>, <a href="https://www.mustafamukadam.com/" target="_blank" rel="noopener noreferrer">Mustafa Mukadam</a>, <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a>, and <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a> </div> <div class="periodical"> <em>CoRL</em> 2021 </div> <div class="links"> <a href="https://openreview.net/pdf?id=nWLt35BU1z_" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://taskography.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/taskography" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/mM4v5hP4LdA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>3D scene graphs (3DSGs) are an emerging description; unifying symbolic, topological, and metric scene representations. However, typical 3DSGs contain hundreds of objects and symbols even for small environments; rendering task planning on the \emphfull graph impractical. We construct \textbfTaskography, the first large-scale robotic task planning benchmark over 3DSGs. While most benchmarking efforts in this area focus on \emphvision-based planning, we systematically study \emphsymbolic planning, to decouple planning performance from visual representation learning. We observe that, among existing methods, neither classical nor learning-based planners are capable of real-time planning over \emphfull 3DSGs. Enabling real-time planning demands progress on \emphboth (a) sparsifying 3DSGs for tractable planning and (b) designing planners that better exploit 3DSG hierarchies. Towards the former goal, we propose \textbfScrub, a task-conditioned 3DSG sparsification method; enabling classical planners to match (and surpass) state-of-the-art learning-based planners. Towards the latter goal, we propose \textbfSeek, a procedure enabling learning-based planners to exploit 3DSG structure, reducing the number of replanning queries required by current best approaches by an order of magnitude. We will open-source all code and baselines to spur further research along the intersections of robot task planning, learning and 3DSGs.</p> </div> <div class="tldr hidden"> <p>We present a large-scale benchmark and performant approaches for long-horizon task planning over large 3D scene graphs</p> </div> <div class="contrib hidden"> <p>Idea was conceived, led, and implemented by Chris and I. Chris focused more on the benchmark. I focused on the SCRUB and SEEK algorithms. Mohamed helped implement several optimal planners. Chris implemented the Taskography-API.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">taskography</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Taskography: Evaluating robot task planning over large 3D scene graphs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Agia, Chris and Jatavallabhula, {Krishna Murthy} and Khodeir, Mohamed and Miksik, Ondrej and Vineet, Vibhav and Mukadam, Mustafa and Paull, Liam and Shkurti, Florian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CoRL}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/gradsim.png" alt="A few physical systems implemented in gradSim."></div> <div class="col-sm"></div> <div id="gradsim" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">gradSim: Differentiable simulation for system identification and visuomotor control</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>*, <a href="http://blog.mmacklin.com/about/" target="_blank" rel="noopener noreferrer">Miles Macklin</a>*, <a href="https://fgolemo.github.io/" target="_blank" rel="noopener noreferrer">Florian Golemo</a>, <a href="https://voletiv.github.io/" target="_blank" rel="noopener noreferrer">Vikram Voleti</a>, <a href="https://lindapetrini.github.io/" target="_blank" rel="noopener noreferrer">Linda Petrini</a>, <a href="http://martincsweiss.com/" target="_blank" rel="noopener noreferrer">Martin Weiss</a>, <a href="http://breandan.net/" target="_blank" rel="noopener noreferrer">Breandan Considine</a>, Jerome Parent-Levesque, <a href="https://kevincxie.github.io/" target="_blank" rel="noopener noreferrer">Kevin Xie</a>, <a href="https://iphys.wordpress.com/" target="_blank" rel="noopener noreferrer">Kenny Erleben</a>, <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a>, <a href="http://www.cs.toronto.edu/~florian/" target="_blank" rel="noopener noreferrer">Florian Shkurti</a>, <a href="https://www.cim.mcgill.ca/~derek/" target="_blank" rel="noopener noreferrer">Derek Nowrouzezahrai</a>, and <a href="https://www.cs.toronto.edu/~fidler/" target="_blank" rel="noopener noreferrer">Sanja Fidler</a> </div> <div class="periodical"> <em>ICLR</em> 2021 </div> <div class="links"> <a href="https://openreview.net/pdf?id=c_E8kFWfhp0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://gradsim.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/gradsim/gradsim" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/YuVdk1b0TVw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>In this paper, we tackle the problem of estimating object physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current best solutions to the problem require precise 3D labels which are labor intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. In this work we present gradSim, a framework that overcomes the dependence on 3D supervision by combining differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This unique combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Furthermore, our unified computation graph across dynamics and rendering engines enables the learning of challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to/better than techniques that require precise 3D labels.</p> </div> <div class="tldr hidden"> <p>Differentiable models of time-varying dynamics and image formation pipelines result in highly accurate physical parameter estimation from video and visuomotor control.</p> </div> <div class="contrib hidden"> <p>This idea was jointly conceived in a meeting which included me, Derek, Breandan, Martin, Bhairav Mehta, and Maxime Chevalier-Boisvert. Martin prototyped an initial differentiable billiards engine. I implemented the first rigid-body engine, integrated it with a differentiable renderer, and setup sys-id experiments. Miles and I then joined forces, with him focusing on the physics engine and me focusing on the physics + rendering combination and overall systems integration. I ran all of the experiments for this paper. Florian (Golemo) and Vikram created the datasets, designed experiments, and also helped with code and the manuscript. All authors participated in writing the manuscript and the author response phase. Florian (Shkurti), Derek, and Sanja nearly equally co-advised on this effort.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gradsim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{gradSim: Differentiable simulation for system identification and visuomotor control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Macklin, Miles and Golemo, Florian and Voleti, Vikram and Petrini, Linda and Weiss, Martin and Considine, Breandan and Parent-Levesque, Jerome and Xie, Kevin and Erleben, Kenny and Paull, Liam and Shkurti, Florian and Nowrouzezahrai, Derek and Fidler, Sanja}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/gradslam.png" alt="An overview of the various components of gradSLAM"></div> <div class="col-sm"></div> <div id="gradslam" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">gradSLAM: Dense SLAM meets automatic differentiation</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>, <a href="https://epiception.github.io/" target="_blank" rel="noopener noreferrer">Ganesh Iyer</a>, and <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a> </div> <div class="periodical"> <em>ICRA</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/pdf/1910.10672.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://gradslam.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/gradslam/gradslam" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=2ygtSJTmo08" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>The question of “representation” is central in the context of dense simultaneous localization and mapping (SLAM). Newer learning-based approaches have the potential to leverage data or task performance to directly inform the choice of representation. However, learning representations for SLAM has been an open question, because traditional SLAM systems are not end-to-end differentiable. In this work, we present gradSLAM, a differentiable computational graph take on SLAM. Leveraging the automatic differentiation capabilities of computational graphs, gradSLAM enables the design of SLAM systems that allow for gradient-based learning across each of their components, or the system as a whole. This is achieved by creating differentiable alternatives for each non-differentiable component in a typical dense SLAM system. Specifically, we demonstrate how to design differentiable trust-region optimizers, surface measurement and fusion schemes, as well as differentiate over rays, without sacrificing performance. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM.</p> </div> <div class="tldr hidden"> <p>We present end-to-end differentiable dense SLAM systems that open up new possibilites for integrating deep learning and SLAM</p> </div> <div class="contrib hidden"> <p>I came up with the idea and led this work. Ganesh Iyer was instrumental with implementing the surfel and point-based fusion pipelines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gradslam</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{gradSLAM: Dense SLAM meets automatic differentiation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Iyer, Ganesh and Paull, Liam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/autolay.png" alt="A preview of the AutoLay benchmark for estimating the bird’s-eye view layout of a road scene."></div> <div class="col-sm"></div> <div id="autolay" class="col-sm-8"> <div class="title">AutoLay: Benchmarking Monocular Layout Estimation</div> <div class="author"> <a href="https://hbutsuak95.github.io/" target="_blank" rel="noopener noreferrer">Kaustubh Mani</a>*, <a href="https://saishankarn.github.io/" target="_blank" rel="noopener noreferrer">Sai Shankar</a>*,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>IROS</em> 2020 </div> <div class="links"> <a href="https://arxiv.org/pdf/2108.09047.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://autolay.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/hbutsuak95/AutoLay" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Amodal layout estimation is the task of estimating a semantic occupancy map in bird’s-eye view, given a monocular image or video. The term amodal implies that we estimate occupancy and semantic labels even for parts of the world that are occluded in image space. In this work, we introduce AutoLay, a new dataset and benchmark for this task. AutoLay provides annotations in 3D, in bird’s-eye view, and in image space. We provide high quality labels for sidewalks, vehicles, crosswalks, and lanes. We evaluate several approaches on sequences from the KITTI and Argoverse datasets.</p> </div> <div class="tldr hidden"> <p>We present a dataset and introduce a new benchmark for *amodal* layout estimation from monocular imagery</p> </div> <div class="contrib hidden"> <p>Kaustubh led this work. I came up with the idea, mentored Kaustubh (admittedly, relatively sparsely), and helped write some of the manuscript. Sai, and particularly Kaustubh, were instrumental in driving this work to completion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">autolay</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AutoLay: Benchmarking Monocular Layout Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mani, Kaustubh and Shankar, Sai and Jatavallabhula, {Krishna Murthy} and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/monolayout.png" alt="Bird’s-eye view layouts estimated by MonoLayout."></div> <div class="col-sm"></div> <div id="monolayout" class="col-sm-8"> <div class="title">MonoLayout: Amodal scene layout from a single image</div> <div class="author"> <a href="https://hbutsuak95.github.io/" target="_blank" rel="noopener noreferrer">Kaustubh Mani</a>, Swapnil Daga, Shubhika Garg, <a href="https://saishankarn.github.io/" target="_blank" rel="noopener noreferrer">Sai Shankar</a>,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>WACV</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/pdf/2002.08394.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://hbutsuak95.github.io/monolayout/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/hbutsuak95/monolayout" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=HcroGyo6yRQ" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>In this paper, we address the novel, highly challenging problem of estimating the layout of a complex urban driving scenario. Given a single color image captured from a driving platform, we aim to predict the bird’s-eye view layout of the road and other traffic participants. The estimated layout should reason beyond what is visible in the image, and compensate for the loss of 3D information due to projection. We dub this problem "amodal scene layout estimation", which involves hallucinating scene layout for even parts of the world that are occluded in the image. To this end, we present MonoLayout, a deep neural network for real-time amodal scene layout estimation from a single image. MonoLayout maps a color image of a scene into a multi-channel occupancy grid in bird’s-eye view, where each channel represents occupancy probabilities of various scene components. We represent scene layout as a multi-channel semantic occupancy grid, and leverage adversarial feature learning to hallucinate plausible completions for occluded image parts. We extend several state-of-the-art approaches for road-layout estimation and vehicle occupancy estimation in bird’s-eye view to the amodal setup and thoroughly evaluate against them. By leveraging temporal sensor fusion to generate training labels, we significantly outperform current art over a number of datasets.</p> </div> <div class="tldr hidden"> <p>We present a neural network that "hallucinates" the layout of a road scene from a single image, including scene parts that are outside the bounds of the image</p> </div> <div class="contrib hidden"> <p>Kaustubh led this work. I came up with the idea, served as mentor, and helped write bulk of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">monolayout</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MonoLayout: Amodal scene layout from a single image}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mani, Kaustubh and Daga, Swapnil and Garg, Shubhika and Shankar, Sai and Jatavallabhula, {Krishna Murthy} and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{WACV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/nair2020iv.png" alt="An illustration of our pipeline involving shape and pose optimization of road vehicles."></div> <div class="col-sm"></div> <div id="nair2020iv" class="col-sm-8"> <div class="title">Multi-object monocular SLAM for dynamic environments</div> <div class="author"> Gokul Nair, Swapnil Daga, <a href="https://scholar.google.com/citations?user=HAtfBjoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Rahul Sajnani</a>, Anirudha Ramesh, <a href="https://junaidcs032.github.io/" target="_blank" rel="noopener noreferrer">Junaid Ahmed Ansari</a>,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>Intelligent Vehicles Symposium (IV)</em> 2020 </div> <font color="red"><b> Finalist - Best Student Paper Award </b></font> <div class="links"> <a href="https://arxiv.org/pdf/2002.03528.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.bilibili.com/video/av90800325/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>In this paper, we tackle the problem of multibody SLAM from a monocular camera. The term multibody, implies that we track the motion of the camera, as well as that of other dynamic participants in the scene. The quintessential challenge in dynamic scenes is unobservability; it is not possible to unambiguously triangulate a moving object from a moving monocular camera. Existing approaches solve restricted variants of the problem, but the solutions suffer relative scale ambiguity (i.e., a family of infinitely many solutions exist for each pair of motions in the scene). We solve this rather intractable problem by leveraging single-view metrology, advances in deep learning, and category-level shape estimation. We propose a multi posegraph optimization formulation, to resolve the relative and absolute scale factor ambiguities involved. This optimization helps us reduce the average error in trajectories of multiple bodies over real-world datasets, such as KITTI. To the best of our knowledge, our method is the first practical monocular multi-body SLAM system to perform dynamic multi-object and ego localization in a unified framework in metric scale.</p> </div> <div class="tldr hidden"> <p>We present a monocular object SLAM system that tracks not just the camera, but also other moving objects in the scene</p> </div> <div class="contrib hidden"> <p>I mentored Gokul and Swapnil on this project. I also wrote a part of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nair2020iv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-object monocular SLAM for dynamic environments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nair, Gokul and Daga, Swapnil and Sajnani, Rahul and Ramesh, Anirudha and Ansari, {Junaid Ahmed} and Jatavallabhula, {Krishna Murthy} and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Intelligent Vehicles Symposium (IV)}</span><span class="p">,</span>
  <span class="na">recognition</span> <span class="p">=</span> <span class="s">{Finalist - Best Student Paper Award}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/maplite.png" alt="An image of the autonomous car (Toyota Prius V) used for MapLite."></div> <div class="col-sm"></div> <div id="maplite" class="col-sm-8"> <div class="title">MapLite: Autonomous intersection navigation without detailed prior maps</div> <div class="author"> Teddy Ort,  <em>Krishna Murthy Jatavallabhula</em>, Rohan Banerjee, <a href="https://saikrishna-1996.github.io/" target="_blank" rel="noopener noreferrer">Sai Krishna Gottipati</a>, <a href="https://dhaivat1729.github.io/" target="_blank" rel="noopener noreferrer">Dhaivat Bhatt</a>, Igor Gilitschenski, <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a>, and Daniela Rus </div> <div class="periodical"> <em>Robotics and Automation Letters</em> 2019 </div> <font color="red"><b> Best paper award </b></font> <div class="links"> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8936918" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/P6Kk5pB2gF4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>In this work, we present MapLite- a one-click autonomous navigation system capable of piloting a vehicle to an arbitrary desired destination point given only a sparse publicly available topometric map (from OpenStreetMap). The onboard sensors are used to segment the road region and register the topometric map in order to fuse the high-level navigation goals with a variational path planner in the vehicle frame. This enables the system to plan trajectories that correctly navigate road intersections without the use of an external localization system such as GPS or a detailed prior map. Since the topometric maps already exist for the vast majority of roads, this solution greatly increases the geographical scope for autonomous mobility solutions. We implement MapLite on a full-scale autonomous vehicle and exhaustively test it on over 15 km of road including over 100 autonomous intersection traversals. We further extend these results through simulated testing to validate the system on complex road junction topologies such as traffic circles.</p> </div> <div class="tldr hidden"> <p>MapLite is a one-click autonomous navigation system for a vehicle that only uses OpenStreetMap data and local sensing</p> </div> <div class="contrib hidden"> <p>Teddy did nearly all of this work. I helped design the topometric registration algorithm and write the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">maplite</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MapLite: Autonomous intersection navigation without detailed prior maps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ort, Teddy and Jatavallabhula, {Krishna Murthy} and Banerjee, Rohan and Gottipati, {Sai Krishna} and Bhatt, Dhaivat and Gilitschenski, Igor and Paull, Liam and Rus, Daniela}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">recognition</span> <span class="p">=</span> <span class="s">{Best paper award}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/kaolin.png" alt="A splash figure showing various features supported by the Kaolin library."></div> <div class="col-sm"></div> <div id="kaolin" class="col-sm-8"> <div class="title"><p style="background-color: lightyellow; color: red">Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research</p></div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>*, <a href="https://edwardsmith1884.github.io/" target="_blank" rel="noopener noreferrer">Edward Smith</a>*, Jean-Francois Lafleche, Clement Fuji Tsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, Rev Lebaredian, and <a href="https://www.cs.toronto.edu/~fidler/" target="_blank" rel="noopener noreferrer">Sanja Fidler</a> </div> <div class="periodical"> <em>Whitepaper</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/pdf/1911.05063.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/NVIDIAGameWorks/kaolin" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Kaolin is a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours.</p> </div> <div class="tldr hidden"> <p>Kaolin is a PyTorch library aimed at accelerating 3D deep learning research.</p> </div> <div class="contrib hidden"> <p>Edward and I led this work during our 2019 internships at NVIDIA. It has since been maintained and developed by several others, notably, Clement, Masha Shugrina, and Towaki Takikawa</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kaolin</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Smith, Edward and Lafleche, Jean-Francois and {Fuji Tsang}, Clement and Rozantsev, Artem and Chen, Wenzheng and Xiang, Tommy and Lebaredian, Rev and Fidler, Sanja}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Whitepaper}</span><span class="p">,</span>
  <span class="na">featured</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/dal.png" alt="An image of the localization experiments (in simulation and on a real robot)."></div> <div class="col-sm"></div> <div id="dal" class="col-sm-8"> <div class="title">Deep Active Localization</div> <div class="author"> <a href="https://saikrishna-1996.github.io/" target="_blank" rel="noopener noreferrer">Sai Krishna Gottipati</a>*, Keehong Seo*,  <em>Krishna Murthy Jatavallabhula</em>, <a href="https://dhaivat1729.github.io/" target="_blank" rel="noopener noreferrer">Dhaivat Bhatt</a>, Vincent Mai, and <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a> </div> <div class="periodical"> <em>Robotics and Automation Letters</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/pdf/1903.01669.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/montrealrobotics/dal" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Active localization is the problem of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches to this use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two modules - a convolutional neural network for perception, and a deep reinforcement learned planning module. We introduce a multi-scale approach to the learned perceptual model since the accuracy needed to perform action selection with reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms using the traditional approach for either perception or planning. We also demonstrate our approaches robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code is also compatible with the OpenAI gym framework, as well as the Gazebo simulator.</p> </div> <div class="tldr hidden"> <p>We demonstrate the applicability of a learned perception model and an exploration policy applied to active localization on real robots</p> </div> <div class="contrib hidden"> <p>Sai and Keehong did most of this work. I was only loosely involved in a mentorship role</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Active Localization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gottipati, {Sai Krishna} and Seo, Keehong and Jatavallabhula, {Krishna Murthy} and Bhatt, Dhaivat and Mai, Vincent and Paull, Liam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics and Automation Letters}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iros2019infer.png" alt="Trajectories forecast in bird’s-eye view by INFER."></div> <div class="col-sm"></div> <div id="infer" class="col-sm-8"> <div class="title">INFER: INtermediate representations for FuturE pRediction</div> <div class="author"> Shashank Srikanth, <a href="https://junaidcs032.github.io/" target="_blank" rel="noopener noreferrer">Junaid Ahmed Ansari</a>, <a href="https://karnikram.info/" target="_blank" rel="noopener noreferrer">Karnik Ram</a>, Sarthak Sharma,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>IROS</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/pdf/1903.10641.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://talsperre.github.io/INFER/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/talsperre/INFER" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=sHxXIX-FZoU" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Deep learning methods have ushered in a new era for computer vision and robotics. With very accurate methods for object detection and semantic segmentation, we are now at a juncture where we can envisage the application of these techniques to perform higher-order understanding. One such application which we consider in this work, is predicting future states of traffic participants in urban driving scenarios. Specifically, we argue that constructing intermediate representations of the world using off-the-shelf computer vision models for semantic segmentation and object detection, we can train models that account for the multi-modality of future states, and at the same time transfer well across different train and test distributions (datasets). Our approach, dubbed INFER (INtermediate representations for distant FuturE pRediction), involves training an autoregressive model that takes in an intermediate representation of past states of the world, and predicts a multimodal distribution over plausible future states. The model consists of an Encoder-Decoder with ConvLSTM present along the skip connections, and in between the Encoder-Decoder. The network takes an intermediate representation of the scene and predicts the future locations of the Vehicle of Interest (VoI). We outperform the current best future prediction model on KITTI while predicting deep into the future (3 sec, 4 sec) by a significant margin. Contrary to most approaches dealing with future prediction that do not generalize well to datasets that they have not been trained on, we test our method on different datasets like Oxford RobotCar and Cityscapes, and show that the network performs well across these datasets which differ in scene layout, weather conditions, and also generalizes well across cross-sensor modalities. We carry out a thorough ablation study on our intermediate representation that captures the role played by different semantics. We conclude the results section by showcasing an important use case of future prediction- multi object tracking and exhibit results on select sequences from KITTI and Cityscapes.</p> </div> <div class="tldr hidden"> <p>INFER demonstrates the applicability of intermediate representations for zero-shot transferrable trajectory forecasting of vehicles in urban driving scenarios</p> </div> <div class="contrib hidden"> <p>Shashank led this work. I came up with the idea, mentored the work, and wrote bulk of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">infer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{INFER: INtermediate representations for FuturE pRediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Srikanth, Shashank and Ansari, {Junaid Ahmed} and Ram, Karnik and Sharma, Sarthak and Jatavallabhula, {Krishna Murthy} and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iros2018.png" alt="An image showing vehicles on steep and graded roads being reconstructed as wireframes, on synthetic as well as real datasets."></div> <div class="col-sm"></div> <div id="ansari2018steep" class="col-sm-8"> <div class="title">The Earth ain’t Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera</div> <div class="author"> <a href="https://junaidcs032.github.io/" target="_blank" rel="noopener noreferrer">Junaid Ahmed Ansari</a>*, Sarthak Sharma*, Anshuman Majumdar,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>IROS</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/pdf/1803.02057.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=KQZFa5_IvpU" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but most such demonstrations have been confined to plain roads. We demonstrate, to the best of our knowledge, the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands as well. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.</p> </div> <div class="tldr hidden"> <p>We demonstrate monocular object localization and wireframe (shape) estimation on extremely steep and graded roads</p> </div> <div class="contrib hidden"> <p>Junaid and Sarthak led this work. I mentored them closely on this project and helped write the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ansari2018steep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Earth ain’t Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ansari, {Junaid Ahmed} and Sharma, Sarthak and Majumdar, Anshuman and Jatavallabhula, {Krishna Murthy} and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/calibnet.png" alt="CalibNet architecture (block diagram) for geometrically-supervised lidar-camera extrinsic calibration."></div> <div class="col-sm"></div> <div id="calibnet" class="col-sm-8"> <div class="title">CalibNet: Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks</div> <div class="author"> <a href="https://epiception.github.io/" target="_blank" rel="noopener noreferrer">Ganesh Iyer</a>, <a href="https://karnikram.info/" target="_blank" rel="noopener noreferrer">Karnik Ram</a>,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>IROS</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/pdf/1803.08181.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/epiception/CalibNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/WyW9T2dSbec" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet - a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation.</p> </div> <div class="tldr hidden"> <p>CalibNet is a geometrically-supervised deep neural network for the extrinsic calibration of lidar-stereo camera rigs</p> </div> <div class="contrib hidden"> <p>Ganesh led this work. I had the initial idea, and Ganesh had clever tweaks that got it to work in practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">calibnet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CalibNet: Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Iyer, Ganesh and Ram, Karnik and Jatavallabhula, {Krishna Murthy} and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ctcnet.png" alt="Neural network architecture for geometrically-supervised visual odometry prediction."></div> <div class="col-sm"></div> <div id="ctcnet" class="col-sm-8"> <div class="title">Geometric Consistency for Self-Supervised End-to-End Visual Odometry</div> <div class="author"> <a href="https://epiception.github.io/" target="_blank" rel="noopener noreferrer">Ganesh Iyer</a>*,  <em>Krishna Murthy Jatavallabhula</em>*, <a href="https://gunshigupta.netlify.app/" target="_blank" rel="noopener noreferrer">Gunshi Gupta</a>, <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a>, and <a href="https://liampaull.ca/" target="_blank" rel="noopener noreferrer">Liam Paull</a> </div> <div class="periodical"> <em>CVPR Workshops</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/pdf/1804.03789.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://krrish94.github.io/CTCNet-release/" class="btn btn-sm z-depth-0" role="button">Webpage</a> <a href="https://github.com/krrish94/CTCNet-release" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.</p> </div> <div class="tldr hidden"> <p>We use the compositional property of transformations to self-supervise learning of visual odometry from images</p> </div> <div class="contrib hidden"> <p>Ganesh contributed to this work more than me. I came up with the idea, but he got this to work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ctcnet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Geometric Consistency for Self-Supervised End-to-End Visual Odometry}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Iyer, Ganesh and Jatavallabhula, {Krishna Murthy} and Gupta, Gunshi and K, {Madhava Krishna} and Paull, Liam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR Workshops}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/beyondpixels.png" alt="Sample multi-object tracking results from our approach when evaluated on the KITTI dataset."></div> <div class="col-sm"></div> <div id="beyondpixels" class="col-sm-8"> <div class="title">Beyond Pixels: Leveraging Geometry and Shape Cues for Multi-Object Tracking</div> <div class="author"> Sarthak Sharma*, <a href="https://junaidcs032.github.io/" target="_blank" rel="noopener noreferrer">Junaid Ahmed Ansari</a>*,  <em>Krishna Murthy Jatavallabhula</em>, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>ICRA</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/pdf/1802.09298.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://junaidcs032.github.io/Geometry_ObjectShape_MOT/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Webpage</a> <a href="https://github.com/JunaidCS032/MOTBeyondPixels" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/SfoK8u2_s-o" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>This paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes.</p> </div> <div class="tldr hidden"> <p>We present a monocular multi-object tracker that uses simple 3D cues and obtained (in 2018) state-of-the-art results.</p> </div> <div class="contrib hidden"> <p>I came up with this idea and mentored Sarthak and Junaid on this work. I also wrote most of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">beyondpixels</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Pixels: Leveraging Geometry and Shape Cues for Multi-Object Tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sharma, Sarthak and Ansari, {Junaid Ahmed} and Jatavallabhula, {Krishna Murthy} and and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/parkhiya2018icra.png" alt="Representative object-SLAM results from the system presented in this paper."></div> <div class="col-sm"></div> <div id="parkhiya2018icra" class="col-sm-8"> <div class="title">Constructing Category-Specific Models for Monocular Object SLAM</div> <div class="author"> <a href="https://parvparkhiya.github.io/" target="_blank" rel="noopener noreferrer">Parv Parkhiya</a>, Rishabh Khawad,  <em>Krishna Murthy Jatavallabhula</em>, <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a>, and <a href="https://sites.google.com/view/brojeshwar/home" target="_blank" rel="noopener noreferrer">Brojeshwar Bhowmick</a> </div> <div class="periodical"> <em>ICRA</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/pdf/1802.09292.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/_LpHrn1opSk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fly, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show — to the best of our knowledge — first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.</p> </div> <div class="tldr hidden"> <p>We present a monocular object SLAM system that uses category-level object representations as object observations</p> </div> <div class="contrib hidden"> <p>Parv implemented bulk of the object SLAM backend. Rishabh implemented the frontend. I proposed this project and mentored Parv and Rishabh, and wrote bulk of the manuscript.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">parkhiya2018icra</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Constructing Category-Specific Models for Monocular Object SLAM}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Parkhiya, Parv and Khawad, Rishabh and Jatavallabhula, {Krishna Murthy} and K, {Madhava Krishna} and Bhowmick, Brojeshwar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2017</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iros2017.png" alt="Car wireframes estimated by the method presented in this paper."></div> <div class="col-sm"></div> <div id="jatavallabhula2017iros" class="col-sm-8"> <div class="title">Shape Priors for Real-Time Monocular Object Localization in Dynamic Environments</div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>, Sarthak Sharma, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>IROS</em> 2017 </div> <div class="links"> <a href="/assets/pdf/iros2017.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/iros2017_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.youtube.com/watch?v=Of-P34rWBBw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>Reconstruction of dynamic objects in a scene is a highly challenging problem in the context of SLAM. In this paper, we present a real-time monocular object localization system that estimates the shape and pose of dynamic objects in real-time, using video frames captured from a moving monocular camera. Although the problem seems to be ill-posed, we demonstrate that, by incorporating prior knowledge of the object category, we can obtain more detailed instance-level reconstructions. As opposed to earlier object model specifications, the proposed shape-prior model leads to the formulation of a Bundle Adjustment-like optimization problem for simultaneous shape and pose estimation. Leveraging recent successes of Convolutional Neural Networks (CNNs) for object keypoint localization, we present a CNN architecture that performs precise keypoint localization. We then demonstrate how these keypoints can be used to recover 3D object properties, while accounting for any 2D localization errors and self-occlusion. We show significant performance improvements compared to state-of-the-art monocular competitors for 2D keypoint detection, as well as 3D localization and reconstruction of dynamic objects.</p> </div> <div class="contrib hidden"> <p>I did most of this work – part of my Masters thesis</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jatavallabhula2017iros</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Shape Priors for Real-Time Monocular Object Localization in Dynamic Environments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Sharma, Sarthak and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/icra2017.png" alt="Car wireframes estimated by the method presented in this paper."></div> <div class="col-sm"></div> <div id="jatavallabhula2017icra" class="col-sm-8"> <div class="title">Reconstructing Vehicles From a Single Image: Shape Priors for Road Scene Understanding</div> <div class="author"> <em>Krishna Murthy Jatavallabhula</em>, <a href="https://saikrishna-1996.github.io/" target="_blank" rel="noopener noreferrer">Sai Krishna Gottipati</a>, Falak Chhaya, and <a href="https://robotics.iiit.ac.in/" target="_blank" rel="noopener noreferrer">Madhava Krishna K</a> </div> <div class="periodical"> <em>ICRA</em> 2017 </div> <div class="links"> <a href="/assets/pdf/icra2017.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/icra2017_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://youtu.be/rSMjHZV4axg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="abstract hidden"> <p>We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in shape priors, which are learnt over a small keypoint-annotated dataset. We then formulate a shape-aware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle keypoints in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches.</p> </div> <div class="contrib hidden"> <p>I did most of this work – part of my Masters thesis</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jatavallabhula2017icra</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reconstructing Vehicles From a Single Image: Shape Priors for Road Scene Understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jatavallabhula, {Krishna Murthy} and Gottipati, {Sai Krishna} and Chhaya, Falak and K, {Madhava Krishna}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2016</h2> <br><br> <ol class="bibliography"><li> <div class="row align-items-center"> <div class="col-sm abbr"><abbr class="badge">JIRS</abbr></div> <div class="col-sm"></div> <div id="fast" class="col-sm-8"> <div class="title">FAST: Frontier Allocation Synchronized by Token-passing</div> <div class="author"> Avinash Gautam, Bhargav Jha, Gourav Kumar,  <em>Krishna Murthy Jatavallabhula</em>, Arjun Ram S P, and Sudeept Mohan </div> <div class="periodical"> <em>Springer Journal of Intelligent Robotic Systems</em> 2016 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="tldr hidden"> <p>We present an efficient, complete, and fault-tolerant multi-robot exploration algorithm</p> </div> <div class="contrib hidden"> <p>I designed and implemented the core algorithm. Authors listed lexicographically by their lastname (my lastname was assumed to be "Murthy") (except for the senior author; listed last)</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fast</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FAST: Frontier Allocation Synchronized by Token-passing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gautam, Avinash and Jha, Bhargav and Kumar, Gourav and Jatavallabhula, {Krishna Murthy} and {S P}, {Arjun Ram} and Mohan, Sudeept}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Springer Journal of Intelligent Robotic Systems}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2015</h2> <br><br> <ol class="bibliography"> <li> <div class="row align-items-center"> <div class="col-sm abbr"><abbr class="badge">SMC</abbr></div> <div class="col-sm"></div> <div id="cac" class="col-sm-8"> <div class="title">Cluster, Allocate, Cover: An Efficient Approach for Multi-robot Coverage</div> <div class="author"> Avinash Gautam,  <em>Krishna Murthy Jatavallabhula</em>, Gourav Kumar, Arjun Ram S P, Bhargav Jha, and Sudeept Mohan </div> <div class="periodical"> <em>IEEE SMC</em> 2015 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="tldr hidden"> <p>We design a performant online multi-robot coverage path planning technique</p> </div> <div class="contrib hidden"> <p>I designed and implemented core aspects of the algorithm</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cac</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cluster, Allocate, Cover: An Efficient Approach for Multi-robot Coverage}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gautam, Avinash and Jatavallabhula, {Krishna Murthy} and Kumar, Gourav and {S P}, {Arjun Ram} and Jha, Bhargav and Mohan, Sudeept}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE SMC}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row align-items-center"> <div class="col-sm abbr"><abbr class="badge">UKSIM</abbr></div> <div class="col-sm"></div> <div id="maxxyt" class="col-sm-8"> <div class="title">Maxxyt: An Autonomous Wearable Device for Real-Time Tracking of a Wide Range of Exercises</div> <div class="author"> Danish Pruthi, Ayush Jain,  <em>Krishna Murthy Jatavallabhula</em>, and Puneet Teja </div> <div class="periodical"> <em>UKSIM</em> 2015 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a class="tldr btn btn-sm z-depth-0" role="button">TL;DR</a> <a class="contrib btn btn-sm z-depth-0" role="button">Contributions</a> </div> <div class="tldr hidden"> <p>We design a wearable device capable of tracking exercise activty entirely on a low-resource microcontroller.</p> </div> <div class="contrib hidden"> <p>I prototyped a few algorithms for tracking and recording repetitions and helped write the manuscript</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">maxxyt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Maxxyt: An Autonomous Wearable Device for Real-Time Tracking of a Wide Range of Exercises}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pruthi, Danish and Jain, Ayush and Jatavallabhula, {Krishna Murthy} and Teja, Puneet}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{UKSIM}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Krishna Murthy Jatavallabhula. Adopted and tweaked the <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: June 14, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-KVVBP90SEH"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-KVVBP90SEH");</script> </body> </html>